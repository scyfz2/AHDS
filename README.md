
# Health Data Science Mini Project 

## Overview

This project automates the process of fetching, cleaning, and analyzing research articles from PubMed based on specific keywords. The automated pipeline includes scripts for data fetching, cleaning article titles, analyzing word frequency, and performing Latent Dirichlet Allocation (LDA) for topic modeling.

## Installation

### Prerequisites

- Unix-based operating system (Linux)
- Python 3.8+
- R programming environment

### Environment Setup

1. Clone the repository:
   ```
   git clone https://github.com/scyfz2/AHDS.git
   ```

2. Install required Python and R packages:
   ```
   conda env create -f environment.yml
   conda activate AHDS-env
   ```

### Configuration

Before running the scripts, ensure that the `config.yaml` file is properly set up with the necessary configurations such as directory paths and processing options.

## Running the Program

To run the entire pipeline, use the provided `Snakefile` which orchestrates the execution of scripts in the correct order:

```
snakemake
```

*Before running snakemake, check the `config` file to make sure the request data is correct.*

Alternatively, individual components can be run as follows:

1. **Fetching Data:**
   ```
   snakemake fetch_data
   ```
    or

   ```
   bash scripts/fetch_data.sh "long covid" 10000 "data/raw/pmids" "data/raw/articles"
   ```

2. **Processing Articles:**
   - Process  XML files:
     ```
     snakemake process_articles
     ```
   - Clean Titles:
     ```
     snakemake clean_titles
     ```
3. **Data Visualisation:**
   - Word Frequency Analysis:
     ```
     snakemake word_frequency
     ```
   - LDA Topic Modeling:
     ```
     snakemake LDA
     ```
4. **Clean Data**
   - remove data, logs and plots directory:
     ```
     snakemake clean
     ```
   - remove plots directory::
     ```
     snakemake cleanplot
     ```
The `data` folder is used to hold the data used by this project.  The directory called `raw` where the raw data is stored. The directory called `clean` where the clean data generated by scripts will go. 

The result of the data visualization will be saved in the `plots` folder, but you can also check out `step4_result` directly, which is a visualization of the sample data.

## Logs

Logs for the data fetching process can be found in the `logs` directory, providing details about the execution and any potential errors.

---

## Modification Record

| Date       | Commit Message                                           |
|------------|----------------------------------------------------------|
| Nov 24, 2024 | Record tag: v2|
| Nov 24, 2024 | Add comments and complete the user manual|
| Nov 23, 2024 | Completed step 4 of the project                         |
| Nov 21, 2024 | Finished implementing steps 1, 2, and 3 of the workflow |
| Nov 21, 2024 | Organized files into their respective directories       |
| Nov 20, 2024 | Record tag: v1|
| Nov 20, 2024 | Addressed the issue of unexpectedly high download speeds |
| Nov 20, 2024 | Implemented steps 2 and 3 of the data processing phase  |
| Nov 20, 2024 | Initiated the project by downloading a set of research articles |
| Nov 14, 2024 | Created the initial README file for the project         |
| Nov 14, 2024 | Made the first commit to the repository                 |

